{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using collab\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras is used for loading the model\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "\n",
    "# We also need sys, os and copy for some processes to retrieve the data (the test set\n",
    "# and model) as well as to install some new modules in colab\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Importing matplotlib, numpy and pandas for handling the test set data and \n",
    "# later for displaying the original image + explanations created by SHAP, Gradcam\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This version is not installed on collab\n",
    "pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing SHAP and keras-vis through pip\n",
    "!{sys.executable} -m pip install shap\n",
    "!{sys.executable} -m pip install keras-vis\n",
    "\n",
    "# Importing SHAP and keras-vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from vis import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a particular Google Drive folder (and place it in the '/content/gdrive'\n",
    "# subdirectory).\n",
    "from google.colab import files, drive\n",
    "drive.mount('YOUR_GOOGLE_DRIVE', force_remount=True) # to ensure that the process can be easily redone in the same runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current operating directory to the Google Drive (which contains our data)\n",
    "os.chdir('YOUR_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example id_list\n",
    "id_list = [12,18,24,29,56,98,106,107]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random layer 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('YOUR_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'dense_1':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap output activation with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'conv2d_94':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random layer 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'conv2d_77':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'conv2d_65':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random layer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'conv2d_51':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: \n",
    "  if hasattr(layer, 'kernel_initializer'):\n",
    "    #print(layer.name, layer)\n",
    "    if layer.name == 'conv2d_1':\n",
    "      layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features2, verbose=1)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df.head())\n",
    "df.to_csv('scram_layer_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model has been scrambled\n",
    "#Regnerate shap / grad  with these images \n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "import copy\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_2')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shap2(features, id_list, model, gmodel, 'Explanations_TEST_Scramble_6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
